{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER MARIO AI #\n",
    "\n",
    "## Welcome to this Exercise! ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/EpB8oRhHSQcnu/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Let's train an agent to play Super Mario!</font>\n",
    "##### We'll be using the Convergent DQN algorithm, a more stable extension of DQN.\n",
    "##### Follow this step-by-step guide and feel to play around with the code. Maybe you'll be able to give Mario an upgrade. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "\n",
    "%pip install keyboard #check out the read me if you're using MacOS\n",
    "%pip install gym\n",
    "%pip install d3rlpy \n",
    "#alternatively: conda install d3rlpy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather data first. After installing the necessary dependencies, run the next cell to collect player data and get a feeling for the game for yourself.\n",
    "To run the following cell, you need to connect to the game server. Therefor, open a terminal and run 'java -jar ./marioai-server-0.1-jar-with-dependencies.jar' in the gym_marioai/gym_marioai/server folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randint\n",
    "import keyboard\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym_marioai import levels\n",
    "\n",
    "\n",
    "all_actions = (0,1,2,3,4,5,6,7,8,9,10,11,12)\n",
    "\n",
    "#create gym environment\n",
    "env = gym.make('Marioai-v0', render=True,\n",
    "               compact_observation=False, #this must stay false for proper saving in dataset\n",
    "               enabled_actions=all_actions,\n",
    "               rf_width=20, rf_height=10)\n",
    "\n",
    "''' \n",
    "0 LEFT\n",
    "1 RIGHT\n",
    "3 DOWN\n",
    "4 JUMP\n",
    "5 SPEED_JUMP\n",
    "6 SPEED_RIGHT\n",
    "7 SPEED_LEFT\n",
    "8 JUMP_RIGHT\n",
    "9 JUMP_LEFT\n",
    "10 SPEED_JUMP_RIGHT\n",
    "11 SPEED_JUMP_LEFT\n",
    "12 NOTHING\n",
    "'''\n",
    "\n",
    "# programmed actions - feel free to change the keys or add actions by yourself\n",
    "def get_action():\n",
    "    if keyboard.is_pressed('up'):\n",
    "        return env.JUMP #4\n",
    "    elif keyboard.is_pressed('right'):\n",
    "        return env.SPEED_RIGHT #6\n",
    "    elif keyboard.is_pressed('left'):\n",
    "        return env.SPEED_LEFT #7\n",
    "    elif keyboard.is_pressed('down'):\n",
    "        return env.DOWN #3\n",
    "   \n",
    "    elif keyboard.is_pressed('d'):\n",
    "        return env.SPEED_JUMP_RIGHT #10\n",
    "    elif keyboard.is_pressed('a'):\n",
    "        return env.SPEED_JUMP_LEFT #11\n",
    "    \n",
    "    else:\n",
    "        return env.NOTHING #12\n",
    "\n",
    "\n",
    "level = levels.coin_level #change to: cliff_level, hard_level, easy_level, coin_level, one_cliff_level, early_cliff_level\n",
    "\n",
    "counter = 0\n",
    "#play loop: execute actions from keyboard input in the environment\n",
    "while True:\n",
    "    #new episode\n",
    "    state = env.reset(level_path=level) \n",
    "    # you can also choose a specific seed, or a random seed\n",
    "    ''' \n",
    "    seed = np.random.randint(0,1000)\n",
    "    state = env.reset(seed=seed)\n",
    "    '''\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    #initialize data arrays with initial states for each episode\n",
    "    observations = [state]\n",
    "    actions = [12] #nothing\n",
    "    rewards = [0]\n",
    "    terminals = [done]\n",
    "\n",
    "    while not done:\n",
    "        action = get_action()\n",
    "        print(action)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        observations.append(next_state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        terminals.append(done)\n",
    "     \n",
    "        total_reward += reward\n",
    "        \n",
    "    #create Markov-Decision-Process Dataset from collected episode\n",
    "    datafile_name = str(level) + \"_\" + \"reward\" + str(int(total_reward)) + \"_\" + str(round(time.time())) \n",
    "    datapath = os.path.join(\"../data\", datafile_name)\n",
    "    \n",
    "    data = np.savez(datapath, observations=observations, actions=actions, rewards=rewards, terminals=terminals)\n",
    "    counter += 1\n",
    "    print(f'finished episode {counter}, total_reward: {total_reward}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could run preprocess_data.py to turn your played games into a Markov Decision Process Dataset. \n",
    "However, the training success largely depends on the number of collected data. No worries, you can use on of the prepared datasets! \n",
    "\n",
    "![](https://media.giphy.com/media/S5uMJDmtnATLbjjw3h/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we will use the Convergent DQN.\n",
    "\n",
    "\n",
    "#### Basics of Deep Q-Learning (DQN)  \n",
    "\n",
    "\n",
    "##### Bellman equation:\n",
    "\n",
    "$Q(s,a;\\theta) = r + \\gamma * max_{a'}Q(s',a';\\~\\theta)$\n",
    "\n",
    "\n",
    "##### Temporal difference (TD) error: \n",
    "The TD-error is the difference between the predicted reward and the actual reward.\n",
    "\n",
    "$\\delta = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta))$\n",
    "\n",
    "\n",
    "##### Huber Loss:\n",
    "To minimize the TD error, we use the Huber Loss as our loss function, which is designed to be more robust to outliers.\n",
    "\n",
    "$L(\\delta) =  \\begin{cases} \\frac{1}{2} * (Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta)))^2 for |\\delta| \\leq \\frac{1}{2} \\\\ |\\delta| - \\frac{1}{2} otherwise  \\end{cases}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergent DQN (CDQN)\n",
    "\n",
    "DQN is a rather simple algorithm, which doesn't always converge. The Convergent DQN (https://arxiv.org/pdf/2106.15419.pdf) ensures loss convergence, by taking the maximum value l_DQN using the target network and l_MSBE (Mean Squared Bellman Error) using the current network. \n",
    "\n",
    "But what does the loss actually mean? And why should it converge?\n",
    "\n",
    "The loss indicates how good or bad the model's prediction was on a sample. If the prediction was perfect, the loss is 0. Otherwise, the loss is larger than 0. \n",
    "Training the model - thus, finding a set of weights and biases - should therefor lower the loss on average over all samples. \n",
    "Note: The loss is a subjective metric depending on your data. A loss of 0.5 can be low for some problems, but large for others. \n",
    "\n",
    "CDQN loss is convergent and performs well in practice. Compared to DQN, CDQN is more stable, independent of data structure. It is defined as follows:\n",
    "\n",
    "\n",
    "$ l\\_DQN = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta))$\n",
    "\n",
    "$ l\\_MSBE = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\theta))$\n",
    "\n",
    "\n",
    "$ l\\_CDQN = {\\mathbb{E}}[max(L\\_DQN, l\\_MSBE)] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_dqn import DQN \n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from constants import DATAPATH\n",
    "import torch\n",
    "\n",
    "### TODO: Please implement the Huber Loss Function from above. Note that 'value' describes the actual cumulated reward and 'target' the predicted cumulated reward.###\n",
    "\n",
    "def huber_loss(beta, gamma, rewards, target, value):\n",
    "  \n",
    "  loss = torch.where(#TODO) \n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, load the dataset and run the DQN algorithm with your implemented loss function ###\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import d3rlpy\n",
    "import gym\n",
    "import gym_marioai\n",
    "from gym_marioai import levels\n",
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "from exercise_dqn import DQN\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from constants import DATAPATH\n",
    "from sklearn.model_selection import train_test_split\n",
    "from d3rlpy.metrics.scorer import td_error_scorer\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\n",
    "\n",
    "dataset = MDPDataset.load(DATAPATH) #TODO: Choose Dataset here\n",
    "\n",
    "dqn = DQN(huber_loss = huber_loss, gamma=0.8, batch_size=128) #TODO: Feel free to experiment with hyperparameters\n",
    "log_dir=\"d3rlpy_logs\"\n",
    "\n",
    "train_episodes, test_episodes = train_test_split(dataset, test_size=0.1) \n",
    "\n",
    "all_actions = (0,1,2,3,4,5,6,7,8,9,10,11,12)\n",
    "\n",
    "env = gym.make('Marioai-v0', render=False, seed=0,\n",
    "               compact_observation=False, #this must stay false for proper saving in dataset\n",
    "               enabled_actions=all_actions,\n",
    "               rf_width=20, rf_height=10)\n",
    "\n",
    "evaluate_scorer = evaluate_on_environment(env)\n",
    "\n",
    "#TODO: experiment with the number of epochs, shuffeling, size of the test set..\n",
    "fitter = cdqn.fitter(train_episodes, eval_episodes=test_episodes, n_epochs=50, shuffle=True, scorers={'environment': evaluate_scorer, 'td_error': td_error_scorer})\n",
    "\n",
    "\n",
    "metr = []\n",
    "for epoch, metrics in fitter:\n",
    "  metr.append(metrics.get('environment'))\n",
    "  #Stop training when a reward over 160 is reached\n",
    "  if metrics.get('environment') > 160:\n",
    "    break\n",
    "  \n",
    "  \n",
    "#fetch latest dataset\n",
    "latest_logs = max(glob.glob(os.path.join(log_dir, '*/')), key=os.path.getmtime)\n",
    "\n",
    "#fetch latest model\n",
    "latest_model = max(glob.iglob(latest_logs + '/*.pt'), key=os.path.getctime)\n",
    "print(latest_model)\n",
    "#to get specific model (not the latest), change this file path\n",
    "cdqn.load_model(latest_model)\n",
    "cdqn.save_policy(latest_logs +'/policy.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the previous cell, you should find a new folder in the log_dir directory. It contains a model for each epoch and some logs, like total rewards in environment.csv. Let's try it out! \n",
    "Choose a model and run the next cell to evaluate the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_marioai import levels\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from constants import DATAPATH\n",
    "from cdqn import CDQN\n",
    "\n",
    "dataset = MDPDataset.load(DATAPATH)\n",
    "\n",
    "\n",
    "### Evaluation of our implemented Convergent DQN algorithm based on the d3rlpy DQN ###\n",
    "cdqn = CDQN()\n",
    "\n",
    "\n",
    "#use this instead of dqn.fit when dqn.fit() has already been run\n",
    "cdqn.build_with_dataset(dataset)\n",
    "\n",
    "#choose your model here\n",
    "cdqn.load_model('../evaluations\\hard_level\\CDQN\\model_15980.pt')\n",
    "\n",
    "all_actions = (0,1,2,3,4,5,6,7,8,9,10,11,12)\n",
    "\n",
    "env = gym.make('Marioai-v0', render=True, # turn this off for fast training without video\n",
    "               level_path=levels.hard_level,\n",
    "               compact_observation=False, #this must stay false for proper saving in dataset\n",
    "               enabled_actions=all_actions,\n",
    "               rf_width=20, rf_height=10)\n",
    "\n",
    "\n",
    "while True:\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "  \n",
    "            action = cdqn.predict([observation])[0]\n",
    "            observation, reward, done, info = env.step(action)\n",
    " \n",
    "       \n",
    "            total_reward += reward\n",
    "        print(f'finished episode, total_reward: {total_reward}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": ""
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
