{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_marioai import levels\n",
    "\n",
    "all_actions = (0,1,2,3,4,5,6,7,8,9,10,11,12)\n",
    "\n",
    "env = gym.make('Marioai-v0', render=True,\n",
    "               level_path=levels.easy_level,\n",
    "               compact_observation=False, #this must stay false for proper saving in dataset\n",
    "               enabled_actions=all_actions,\n",
    "               rf_width=20, rf_height=10)\n",
    "while True:\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    #initialize data arrays with initial states for each episode\n",
    "    observations = [env.reset(seed=0)]\n",
    "    actions = [12] #nothing\n",
    "    rewards = [0]\n",
    "    terminals = [done]\n",
    "\n",
    "    while not done:\n",
    "        action = 3\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        observations.append(next_state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        terminals.append(done)\n",
    "     \n",
    "        total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we have stored the transitions into the experience replay memory, mapping the (state, action)-pairs to their (next-state, reward).\n",
    "Now, let's train a policy to maximize the discounted, cumulative reward (a.k.a return). For this, we use the DQN algorithm:\n",
    "\n",
    "### Deep Q-Learning (DQN)  \n",
    "\n",
    "#### Bellman equation:\n",
    "\n",
    "$Q(s,a;\\theta) = r + \\gamma * max_{a'}Q(s',a';\\~\\theta)$\n",
    "\n",
    "\n",
    "#### Temporal difference (TD) error: \n",
    "The TD-error is the difference between the predicted reward and the actual reward.\n",
    "\n",
    "$\\delta = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta))$\n",
    "\n",
    "\n",
    "#### Huber Loss:\n",
    "To minimize the TD error, we use the Huber Loss as our loss function, which is designed to be more robust to outliers.\n",
    "\n",
    "$L(\\delta) =  \\begin{cases} \\frac{1}{2} * (Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta)))^2 for |\\delta| \\leq \\frac{1}{2} \\\\ |\\delta| - \\frac{1}{2} otherwise  \\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_dqn import DQN \n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from constants import DATAPATH\n",
    "import torch\n",
    "\n",
    "### TODO: Please implement the Huber Loss Function from above. Note that 'value' describes the actual cumulated reward and 'target' the predicted cumulated reward.###\n",
    "\n",
    "def huber_loss(beta, gamma, rewards, target, value):\n",
    "  \n",
    "  y = rewards + gamma * target\n",
    "  diff = (value - y)\n",
    "  cond = diff.detach().abs() < beta\n",
    "  loss = torch.where(cond, 0.5 * diff**2, beta * (diff.abs() - 0.5 * beta))\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, load the dataset and run the DQN algorithm with your implemented loss function ###\n",
    "\n",
    "\n",
    "dataset = MDPDataset.load(DATAPATH)\n",
    "\n",
    "dqn = DQN(huber_loss = huber_loss)\n",
    "\n",
    "dqn.fit(dataset, n_epochs=10) #feel free to adjust the number of complete passes through the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergend DQN (CDQN)\n",
    "\n",
    "DQN is a rather simple algorithm, which doesn't always converge. The Convergent DQN (https://arxiv.org/pdf/2106.15419.pdf) ensures loss convergence, by taking the maximum value l_DQN using the target network and l_MSBE (Mean Squared Bellman Error) using the current network. \n",
    "\n",
    "\n",
    "\n",
    "$ l\\_DQN = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta))$\n",
    "\n",
    "$ l\\_MSBE = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\theta))$\n",
    "\n",
    "\n",
    "$ l\\_CDQN = {\\mathbb{E}}[max(L_DQN, l_MSBE)] $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, run the CDQN and compare the loss in loss.txt ###\n",
    "\n",
    "from cdqn import CDQN \n",
    "from constants import DATAPATH\n",
    "\n",
    "cdqn = CDQN()\n",
    "\n",
    "dataset = MDPDataset.load(DATAPATH)\n",
    "\n",
    "cdqn.fit(train_episodes, eval_episodes=test_episodes, n_epochs=15, logdir=log_dir, save_interval=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super Mario Evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
