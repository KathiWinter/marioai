{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER MARIO AI #\n",
    "\n",
    "## Welcome to this Exercise! ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/EpB8oRhHSQcnu/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Let's train an agent to play Super Mario!</font>\n",
    "##### We'll be using the Deep Q-Learning (DQN) algorithm.\n",
    "##### Follow this step-by-step guide and feel to play around with the code. Maybe you'll be able to give Mario an upgrade. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "\n",
    "%pip install keyboard #check out the read me if you're using MacOS\n",
    "%pip install gym\n",
    "%pip install d3rlpy \n",
    "#alternatively: conda install d3rlpy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather data first. After installing the necessary dependencies, run the next cell to collect player data and get a feeling for the game for yourself.\n",
    "To run the following cell, you need to connect to the game server. Therefor, open a terminal and run 'java -jar ./marioai-server-0.1-jar-with-dependencies.jar' in the gym_marioai/gym_marioai/server folder.\n",
    "\n",
    "\n",
    "We encourage you to generate datasets at different levels. These levels include:\n",
    "+ cliffLevel\n",
    "+ coinLevel\n",
    "+ earlyCliffLevel\n",
    "+ easyLevel\n",
    "+ enemyLevel\n",
    "+ flatLevel\n",
    "+ hardLevel\n",
    "+ oneCliffLevel\n",
    "\n",
    "There are more detailed instructions below that may help you generate the data.\n",
    "\n",
    "+ `python play-for-training.py -u <int>` or `python play-for-training.py --user <int>` sets the user flag when collecting data for training. 0 = test user we will ignore all other: user-ids\n",
    "+ `python play-for-training.py` runs the default seed 0\n",
    "+ `python play-for-training.py -l coinLevel` or `python play-for-training.py --level coinLevel` runs specified level 'coinLevel'\n",
    "+ `python play-for-training.py -s 188` or `python play-for-training.py --seed 188` runs specified seed 188\n",
    "+ `python play-for-training.py -s random` or `python play-for-training.py --seed random` runs new random seed for each episode.\n",
    "+ `python play-for-training.py --level coinLevel --seed 188` When a level is specified, python runs specified level and ignores seed number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "#set different commands according to your needs\n",
    "command = \"sudo -S python play-for-training.py\"\n",
    "os.system('echo %s | %s' % (password, command))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to enrich your dataset, you can run random-for-training.py to add random data to the dataset.\n",
    "\n",
    "+ `!python random-for-training.py` runs the default seed 0\n",
    "+ `!python random-for-training.py -l coinLevel` or `!python random-for-training.py --level coinLevel` runs specified level 'coinLevel'\n",
    "+ `!python random-for-training.py -s 188` or `!python random-for-training.py --seed 188` runs specified seed 188\n",
    "+ `!python random-for-training.py -s random` or `!python random-for-training.py --seed random` runs new random seed for each episode.\n",
    "\n",
    "the generated data willl be stored in ‘/data’ as npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python random-for-training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, You need to run preprocess_data.py to turn your played games into a Markov Decision Process Dataset. \n",
    "However, the training success largely depends on the number of collected data. \n",
    "\n",
    "No worries, you can use on of the prepared datasets! \n",
    "\n",
    "![](https://media.giphy.com/media/S5uMJDmtnATLbjjw3h/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Basics of Deep Q-Learning (DQN)  \n",
    "\n",
    "In deterministic environments, DQN approximates the return of a state x action pair. For function updates, every policy obeys the Bellman equation:\n",
    "\n",
    "$Q(s,a;\\theta) = r + \\gamma * max_{a'}Q(s',a';\\~\\theta)$\n",
    "\n",
    "\n",
    "The TD-error is the difference between the predicted reward and the actual reward.\n",
    "\n",
    "$\\delta = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta))$\n",
    "\n",
    "\n",
    "To minimize the TD error, we use the Huber Loss as our loss function, which is designed to be more robust to outliers.\n",
    "\n",
    "$L(\\delta) =  \\begin{cases} \\frac{1}{2} * (Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta)))^2 for |\\delta| \\leq \\frac{1}{2} \\\\ |\\delta| - \\frac{1}{2} otherwise  \\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_dqn import DQN \n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from constants import DATAPATH\n",
    "import torch\n",
    "\n",
    "### TODO: Please implement the Huber Loss Function from above. Note that 'value' describes the actual cumulated reward and 'target' the predicted cumulated reward.###\n",
    "\n",
    "def huber_loss(beta, gamma, rewards, target, value):\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "  loss = torch.where() #TODO \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the dataset and run the DQN algorithm with your implemented loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, load the dataset and run the DQN algorithm with your implemented loss function ###\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import d3rlpy\n",
    "import gym\n",
    "import gym_marioai\n",
    "from gym_marioai import levels\n",
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "from exercise_dqn import DQN\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from constants import DATAPATH\n",
    "from sklearn.model_selection import train_test_split\n",
    "from d3rlpy.metrics.scorer import td_error_scorer\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\n",
    "\n",
    "dataset = MDPDataset.load(DATAPATH) #Choose Dataset here\n",
    "\n",
    "dqn = DQN(huber_loss = huber_loss, gamma=0.8, batch_size=128) #TODO: Feel free to experiment with hyperparameters\n",
    "log_dir=\"d3rlpy_logs\"\n",
    "\n",
    "train_episodes, test_episodes = train_test_split(dataset, test_size=0.1) \n",
    "\n",
    "all_actions = (0,1,2,3,4,5,6,7,8,9,10,11,12)\n",
    "\n",
    "env = gym.make('Marioai-v0', render=False,\n",
    "               level_path=levels.coin_level,\n",
    "               compact_observation=False, #this must stay false for proper saving in dataset\n",
    "               enabled_actions=all_actions,\n",
    "               rf_width=20, rf_height=10)\n",
    "\n",
    "evaluate_scorer = evaluate_on_environment(env)\n",
    "\n",
    "#TODO: experiment with the number of epochs, shuffeling, size of the test set..\n",
    "fitter = dqn.fitter(train_episodes, eval_episodes=test_episodes, n_epochs=20, shuffle=True, scorers={'environment': evaluate_scorer, 'td_error': td_error_scorer})\n",
    "\n",
    "\n",
    "metr = []\n",
    "for epoch, metrics in fitter:\n",
    "  metr.append(metrics.get('environment'))\n",
    "  #Stop training when a reward over 160 is reached\n",
    "  if metrics.get('environment') > 160:\n",
    "    break\n",
    "  \n",
    "  \n",
    "#fetch latest dataset\n",
    "latest_logs = max(glob.glob(os.path.join(log_dir, '*/')), key=os.path.getmtime)\n",
    "\n",
    "#fetch latest model\n",
    "latest_model = max(glob.iglob(latest_logs + '/*.pt'), key=os.path.getctime)\n",
    "print(latest_model)\n",
    "#to get specific model (not the latest), change this file path\n",
    "dqn.load_model(latest_model)\n",
    "dqn.save_policy(latest_logs +'/policy.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the previous cell, you should find a new folder in the log_dir directory. It contains a model for each epoch and some logs, like total rewards in environment.csv. Let's try it out! \n",
    "Choose a model and run the next cell to evaluate the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_marioai import levels\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from constants import DATAPATH\n",
    "from exercise_dqn import DQN\n",
    "\n",
    "dataset = MDPDataset.load(DATAPATH)\n",
    "\n",
    "\n",
    "### Evaluation of your model on d3rlpy DQN###\n",
    "dqn = DQN(huber_loss = huber_loss)\n",
    "\n",
    "\n",
    "#use this instead of dqn.fit when dqn.fit() has already been run\n",
    "dqn.build_with_dataset(dataset)\n",
    "\n",
    "#TODO choose your model here\n",
    "dqn.load_model('')\n",
    "\n",
    "all_actions = (0,1,2,3,4,5,6,7,8,9,10,11,12)\n",
    "\n",
    "env = gym.make('Marioai-v0', render=True, # turn this off for fast training without video\n",
    "               level_path=levels.coin_level,\n",
    "               compact_observation=False, #this must stay false for proper saving in dataset\n",
    "               enabled_actions=all_actions,\n",
    "               rf_width=20, rf_height=10)\n",
    "\n",
    "\n",
    "while True:\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "  \n",
    "            action = dqn.predict([observation])[0]\n",
    "            observation, reward, done, info = env.step(action)\n",
    " \n",
    "       \n",
    "            total_reward += reward\n",
    "        print(f'finished episode, total_reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! But can you do better?\n",
    "Thanks for the visit!\n",
    "\n",
    "![](https://tenor.com/view/mario-pipe-byebye-gif-5530137.gif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6709c8e5a7c6627d3f357d9d1d1387f6958bf8c6958f5cd1ae19a9272f28994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
