{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_marioai import levels\n",
    "\n",
    "all_actions = (0,1,2,3,4,5,6,7,8,9,10,11,12)\n",
    "\n",
    "env = gym.make('Marioai-v0', render=True,\n",
    "               level_path=levels.easy_level,\n",
    "               compact_observation=False, #this must stay false for proper saving in dataset\n",
    "               enabled_actions=all_actions,\n",
    "               rf_width=20, rf_height=10)\n",
    "while True:\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    #initialize data arrays with initial states for each episode\n",
    "    observations = [env.reset(seed=0)]\n",
    "    actions = [12] #nothing\n",
    "    rewards = [0]\n",
    "    terminals = [done]\n",
    "\n",
    "    while not done:\n",
    "        action = 3\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        observations.append(next_state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        terminals.append(done)\n",
    "     \n",
    "        total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player generated data\n",
    "\n",
    "Let's generate some data for training! (If you are not interested in generating your own data, you can also use our generated dataset.)\n",
    "We encourage you to generate datasets at different levels. These levels include: \n",
    "+ cliffLevel\n",
    "+ coinLevel\n",
    "+ earlyCliffLevel\n",
    "+ easyLevel\n",
    "+ enemyLevel\n",
    "+ flatLevel\n",
    "+ hardLevel\n",
    "+ oneCliffLevel\n",
    "\n",
    "There are more detailed instructions below that may help you generate the data.\n",
    "+ `python play-for-training.py -u <int>` or `python play-for-training.py --user <int>` sets the user flag when collecting data for training.\n",
    "0 = test user we will ignore\n",
    "all other: user-ids\n",
    "+ `python play-for-training.py` runs the default seed 0\n",
    "+ `python play-for-training.py -l coinLevel` or `python play-for-training.py --level coinLevel` runs specified level 'coinLevel'\n",
    "+ `python play-for-training.py -s 188` or `python play-for-training.py --seed 188` runs specified seed 188\n",
    "+ `python play-for-training.py -s random` or `python play-for-training.py --seed random` runs new random seed for each episode. \n",
    "+ `python play-for-training.py --level coinLevel --seed 188` When a level is specified, python runs specified level and ignores seed number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import getpass\n",
    " import os\n",
    "\n",
    " password = getpass.getpass()\n",
    " #set different commands according to your needs\n",
    " command = \"sudo -S python play-for-training.py\"\n",
    " os.system('echo %s | %s' % (password, command))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random generated data\n",
    "It's possible to generate some random data to enrich offline dataset.\n",
    "\n",
    "+ `!python random-for-training.py` runs the default seed 0\n",
    "+ `!python random-for-training.py -l coinLevel` or `!python random-for-training.py --level coinLevel` runs specified level 'coinLevel'\n",
    "+ `!python random-for-training.py -s 188` or `!python random-for-training.py --seed 188` runs specified seed 188\n",
    "+ `!python random-for-training.py -s random` or `!python random-for-training.py --seed random` runs new random seed for each episode.\n",
    "\n",
    "the generated data willl be stored in ‘/data/random_Generated_Episode’ as npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python random-for-training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we have stored the transitions into the experience replay memory, mapping the (state, action)-pairs to their (next-state, reward).\n",
    "Now, let's train a policy to maximize the discounted, cumulative reward (a.k.a return). For this, we use the DQN algorithm:\n",
    "\n",
    "### Deep Q-Learning (DQN)  \n",
    "\n",
    "#### Bellman equation:\n",
    "\n",
    "$Q(s,a;\\theta) = r + \\gamma * max_{a'}Q(s',a';\\~\\theta)$\n",
    "\n",
    "\n",
    "#### Temporal difference (TD) error: \n",
    "The TD-error is the difference between the predicted reward and the actual reward.\n",
    "\n",
    "$\\delta = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta))$\n",
    "\n",
    "\n",
    "#### Huber Loss:\n",
    "To minimize the TD error, we use the Huber Loss as our loss function, which is designed to be more robust to outliers.\n",
    "\n",
    "$L(\\delta) =  \\begin{cases} \\frac{1}{2} * (Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta)))^2 for |\\delta| \\leq \\frac{1}{2} \\\\ |\\delta| - \\frac{1}{2} otherwise  \\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_dqn import DQN \n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from constants import DATAPATH\n",
    "import torch\n",
    "\n",
    "### TODO: Please implement the Huber Loss Function from above. Note that 'value' describes the actual cumulated reward and 'target' the predicted cumulated reward.###\n",
    "\n",
    "def huber_loss(beta, gamma, rewards, target, value):\n",
    "  \n",
    "  loss = torch.where() #TODO\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, load the dataset and run the DQN algorithm with your implemented loss function ###\n",
    "\n",
    "\n",
    "dataset = MDPDataset.load(DATAPATH)\n",
    "\n",
    "dqn = DQN(huber_loss = huber_loss)\n",
    "\n",
    "dqn.fit(dataset, n_epochs=10) #feel free to adjust the number of complete passes through the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergent DQN (CDQN)\n",
    "\n",
    "DQN is a rather simple algorithm, which doesn't always converge. The Convergent DQN (https://arxiv.org/pdf/2106.15419.pdf) ensures loss convergence, by taking the maximum value l_DQN using the target network and l_MSBE (Mean Squared Bellman Error) using the current network. \n",
    "\n",
    "\n",
    "\n",
    "$ l\\_DQN = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\~\\theta))$\n",
    "\n",
    "$ l\\_MSBE = Q(s,a;\\theta) - (r + \\gamma * max_{a'}Q(s',a';\\theta))$\n",
    "\n",
    "\n",
    "$ l\\_CDQN = {\\mathbb{E}}[max(L_DQN, l_MSBE)] $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, run the CDQN and compare the loss in loss.txt. What do you observe?  ###\n",
    "### What does the loss tell about the training? Why shouldn't it be used as the only indicator of a good training? ###\n",
    "\n",
    "from cdqn import CDQN \n",
    "from constants import DATAPATH\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cdqn = CDQN()\n",
    "\n",
    "dataset = MDPDataset.load(DATAPATH)\n",
    "log_dir=\"d3rlpy_logs\"\n",
    "\n",
    "train_episodes, test_episodes = train_test_split(dataset, test_size=0.1) \n",
    "\n",
    "cdqn.fit(train_episodes, eval_episodes=test_episodes, n_epochs=15, logdir=log_dir, save_interval=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super Mario Evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6709c8e5a7c6627d3f357d9d1d1387f6958bf8c6958f5cd1ae19a9272f28994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
